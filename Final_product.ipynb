{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae06ebba",
   "metadata": {},
   "source": [
    "Run all cells for the model to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf7e7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from peft import get_peft_model, LoraConfig, TaskType, PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94373347",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bert\n",
    "BERT_MODEL_NAME = \"cl-tohoku/bert-base-japanese\"\n",
    "\n",
    "# ffnn\n",
    "FFNN_MODEL_PATH = \"models/tone_classifier.pt\"\n",
    "\n",
    "# gpt\n",
    "LORA_MODEL_PATH = \"models/rinna-lora-finetuned\"\n",
    "GPT_MODEL_NAME = \"rinna/japanese-gpt2-medium\"\n",
    "\n",
    "# dataset files for labels\n",
    "TONE_TRAIN_FILE_1 = 'data/jchat_paired.csv' \n",
    "TONE_TRAIN_FILE_2 = 'data/chigiri_train_w_tone.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "31d245a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained models\n",
    "# bert (bert tokenizer for FFNN tone prediction)\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL_NAME, use_fast=False)\n",
    "bert_model = AutoModel.from_pretrained(BERT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b997f464",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gpt_tokenize_function(data, max_length=512):\n",
    "    \"\"\"\n",
    "    Tokenize a sentence.\n",
    "    \n",
    "    Args:\n",
    "        data: a row from dataset\n",
    "        max_length: maximum length of returned result\n",
    "        \n",
    "    Return:\n",
    "        tokenized: tokenized sentence\n",
    "    \"\"\"\n",
    "    # reformat the prompts into a full sentnece\n",
    "    prompts = [\n",
    "        instruction + \"<NL>\" + user + \"<NL>\" + output + gpt_tokenizer.eos_token\n",
    "        for instruction, user, output in zip(data[\"instruction\"], data[\"input\"], data[\"output\"])\n",
    "    ]\n",
    "\n",
    "    # tokenize using the tokenizer\n",
    "    tokenized = gpt_tokenizer(prompts, truncation=True, padding=\"max_length\", max_length=max_length)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "\n",
    "    return tokenized\n",
    "\n",
    "def load_trained_LoRA(base_model, lora_path):\n",
    "    \"\"\"\n",
    "    Load in the trained LoRA layers to the base model.\n",
    "    \n",
    "    Args:\n",
    "       base_model: base gpt model. Should be the same one as LoRA was trained\n",
    "       lora_path: LORA_MODEL_PATH\n",
    "       \n",
    "    Return:\n",
    "        combined_full_model\n",
    "    \"\"\"\n",
    "    return PeftModel.from_pretrained(base_model, lora_path, is_trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "928aa6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
     ]
    }
   ],
   "source": [
    "# gpt\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(GPT_MODEL_NAME, use_fast=False)\n",
    "gpt_tokenizer.add_special_tokens({\"additional_special_tokens\": [\"###指示:\", \"###ユーザー:\", \"###キャラ:\"]})\n",
    "gpt_base_model = AutoModelForCausalLM.from_pretrained(GPT_MODEL_NAME)\n",
    "gpt_base_model.resize_token_embeddings(len(gpt_tokenizer))\n",
    "\n",
    "full_gpt_model = load_trained_LoRA(gpt_base_model, LORA_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f1aa12e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['anger',\n",
       " 'disgust',\n",
       " 'fear',\n",
       " 'joy',\n",
       " 'neutral',\n",
       " 'sadness',\n",
       " 'surprise',\n",
       " 'ツンデレっぽく、素直じゃないけど質問には答える',\n",
       " 'ネタっぽく・軽いノリでふざけて',\n",
       " '冷静に説明・論理的に助言・分析する',\n",
       " '前向きで熱く、感情的になっている',\n",
       " '心を開きたくないけど、実は話してる',\n",
       " '怒っている・挑発的・強く主張する',\n",
       " '悔しさと怒りを爆発させる',\n",
       " '普通の会話・挨拶っぽく軽い返事・気分が良い',\n",
       " '激しい語気で挑発',\n",
       " '短く突っ込む・反射的なリアクション',\n",
       " '素直じゃない・気分屋・ワガママ・理不尽的',\n",
       " '緊張・心の声・ためらい',\n",
       " '自信満々に・堂々と・しどろもどろに',\n",
       " '自分の話を語る・心情を吐露する',\n",
       " '言いたくない・感情を隠す・葛藤をにじませる']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FFNN\n",
    "# get out all tone from both files\n",
    "labels = set()\n",
    "for file_path in [TONE_TRAIN_FILE_1, TONE_TRAIN_FILE_2]:\n",
    "    df_data = pd.read_csv(file_path)\n",
    "    labels = labels | set(df_data[\"tone\"])\n",
    "\n",
    "# sort the labels so it will be the same every time\n",
    "ls_label = sorted(list(labels))\n",
    "ls_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "484202b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_translate_tone = {\"anger\": \" 怒っている\",\n",
    "                      \"disgust\": \"嫌悪感を持っている\",\n",
    "                      \"fear\": \"怯えている\",\n",
    "                      \"joy\": \"嬉しそうな\",\n",
    "                      \"neutral\": \"落ち着いた\",\n",
    "                      \"sadness\": \"悲しそう\",\n",
    "                      \"surprise\": \"驚いている\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1410f431",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FFNN Model loaded\n"
     ]
    }
   ],
   "source": [
    "class FFNN(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_units=128, num_classes=len(labels)):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_units)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_units, num_classes)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.fc1(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.dropout(X)\n",
    "        return self.fc2(X)\n",
    "\n",
    "def load_model(path, input_dim=768, hidden_units=128, num_classes=len(labels)):\n",
    "    \"\"\"Load model parameters from file and return the model.\"\"\"\n",
    "    model = FFNN(input_dim=input_dim, hidden_units=hidden_units, num_classes=num_classes)\n",
    "    model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "    model.eval()\n",
    "    print(f\"FFNN Model loaded\")\n",
    "    return model\n",
    "\n",
    "ffnn_model = load_model(FFNN_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe095f3",
   "metadata": {},
   "source": [
    "# Full function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "517bf007",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_tone(text, ffnn_model, bert_tokenizer, bert_embedding, ls_label):\n",
    "    \"\"\"\n",
    "    Use the FFNN model to predict response tone.\n",
    "    \n",
    "    Args:\n",
    "        text: plain user inputted text\n",
    "        ffnn_model: the trained FFNN model\n",
    "        bert_tokenizer: loaded Tohoku bert tokenizer\n",
    "        bert_embedding: loaded Tohoku bert embedding model\n",
    "        ls_label: a list of labels of tone\n",
    "        \n",
    "    Return:\n",
    "        predicted tone\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # tokenize and do embedding\n",
    "        inputs = bert_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "        outputs = bert_embedding(**inputs)\n",
    "        embedding = outputs.last_hidden_state[:, 0, :]\n",
    "        \n",
    "        # get prediction\n",
    "        logits = ffnn_model(embedding)\n",
    "        pred_idx = torch.argmax(logits, dim=1).item()\n",
    "        \n",
    "    return ls_label[pred_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ef25e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_char_response(include_tone=True):\n",
    "    \"\"\"\n",
    "    Full function to generate character's response. Ask the user to give input\n",
    "    and generate response using the created model.\n",
    "    \n",
    "    Args:\n",
    "        include_tone (bool): whether or not to use the FFNN model to predict responsing tone\n",
    "    \"\"\"\n",
    "    # ask user to input text\n",
    "    user_input = input()\n",
    "    \n",
    "    # step 1: use bert to tokenized, embedding, and use\n",
    "    if include_tone:\n",
    "        pred_tone = predict_tone(user_input, ffnn_model, bert_tokenizer, bert_model, ls_label)\n",
    "        \n",
    "        # tone in j-chat are in English, change to japanese\n",
    "        if pred_tone in dict_translate_tone:\n",
    "            reply_tone = dict_translate_tone[pred_tone]\n",
    "        else:\n",
    "            reply_tone = pred_tone\n",
    "    \n",
    "        # combint tone to the front of the user input\n",
    "        user_input = pred_tone + \"口調で返事をください。<NL>ユーザー:\" + user_input + \"<NL>キャラ:\"\n",
    "    else:\n",
    "        user_input = \"ユーザー:\" + user_input\n",
    "        \n",
    "    # step 2: put into fine_tuned rinna/gpt and generat response\n",
    "    inputs = gpt_tokenizer(user_input, return_tensors=\"pt\").to(full_gpt_model.device)\n",
    "    output = full_gpt_model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        eos_token_id=gpt_tokenizer.eos_token_id,\n",
    "        pad_token_id=gpt_tokenizer.pad_token_id,\n",
    "        repetition_penalty=1.2,\n",
    "    )\n",
    "\n",
    "    # step 3: print out the result\n",
    "    print(gpt_tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d6ee46",
   "metadata": {},
   "source": [
    "# Test the performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9c489e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "君の武器を教えてくれ。\n",
      "冷静に説明・論理的に助言・分析する口調で返事をください。<>ユーザー:君の武器を教えてくれ。<>キャラ:##キャラ: 俺はお前が最強だと思ってるからな! ##キャラ: 俺はお前の強さを信じてるんだよ\n"
     ]
    }
   ],
   "source": [
    "# test for response with tone\n",
    "generate_char_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "234c5ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "君の武器を教えてくれ。\n",
      "ユーザー:君の武器を教えてくれ。##キャラ: 俺は、お前に勝てる気がしないんだよ! ##キャラ: いや、俺もそう\n"
     ]
    }
   ],
   "source": [
    "# test for response without tone\n",
    "generate_char_response(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "942e8750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザー: 君の武器を教えてくれ。キャラ:僕は、このゲームをプレイするために必要なものを全て持っているんだ。 君が望むなら、僕はこのゲームで得た知識と経験を使って、君に教えてあげるよ。 君は、このゲームで得た知識と経験を使って\n"
     ]
    }
   ],
   "source": [
    "# compair with not-fine-tuned model\n",
    "input_text = \"ユーザー: 君の武器を教えてくれ。キャラ:\"\n",
    "\n",
    "inputs = gpt_tokenizer(input_text, return_tensors=\"pt\").to(gpt_base_model.device)\n",
    "output = gpt_base_model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=50,\n",
    "    eos_token_id=gpt_tokenizer.eos_token_id,\n",
    "    pad_token_id=gpt_tokenizer.pad_token_id,\n",
    "    repetition_penalty=1.2,\n",
    ")\n",
    "print(gpt_tokenizer.decode(output[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b69fffe",
   "metadata": {},
   "source": [
    "#### Try for several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8902976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With FFNN tone:\n",
      "世界一のストライカー‬になりたいんだ\n",
      "冷静に説明・論理的に助言・分析する口調で返事をください。<>ユーザー:世界一のストライカーになりたいんだ<>キャラ:4||0||||0||0||0||||4||なが=\"2\"|-||0||0||2||1本が3||0||3||1にする=\"2\"|1||0||=2|-||||5||2||0の町0||0||1||0=\"2\"|-||『\"\"』0||0||||||||||||||||0||1拮\n",
      "\n",
      "Without FFNN tone:\n",
      "世界一のストライカー‬になりたいんだ\n",
      "ユーザー:世界一のストライカーになりたいんだ###キャラ: お前は俺に勝てるのか? ##キャラ: お前が俺を倒せるか! #\n"
     ]
    }
   ],
   "source": [
    "print(\"With FFNN tone:\")\n",
    "generate_char_response()\n",
    "print()\n",
    "print(\"Without FFNN tone:\")\n",
    "generate_char_response(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "25c2a417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With FFNN tone:\n",
      "怪我した時、正直どんな気持ちだった？\n",
      "冷静に説明・論理的に助言・分析する口調で返事をください。<>ユーザー:怪我した時、正直どんな気持ちだった?<>キャラ:###キャラ: 俺はお前の味方だ! ##キャラ: お前がそう思ってくれるならそれでいいんだ\n",
      "\n",
      "Without FFNN tone:\n",
      "怪我した時、正直どんな気持ちだった？\n",
      "ユーザー:怪我した時、正直どんな気持ちだった?4||0|| 当日有権者数:0人/1人 ###キャラ: 俺はお前の味方だ! ##キャラ: お前\n"
     ]
    }
   ],
   "source": [
    "print(\"With FFNN tone:\")\n",
    "generate_char_response()\n",
    "print()\n",
    "print(\"Without FFNN tone:\")\n",
    "generate_char_response(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "502d8952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With FFNN tone:\n",
      "君にとって「ストライカー」って何？\n",
      "冷静に説明・論理的に助言・分析する口調で返事をください。<>ユーザー:君にとって「ストライカー」って何?<>キャラ:ループしてるの? ###キャラ: 俺はお前がストライカーだと思ってるからな! ##キャラ: い\n",
      "\n",
      "Without FFNN tone:\n",
      "君にとって「ストライカー」って何？\n",
      "ユーザー:君にとって「ストライカー」って何?###キャラ: お前は俺のヒーローだ! ##キャラ: お前が俺を倒せるなら、俺も倒\n"
     ]
    }
   ],
   "source": [
    "print(\"With FFNN tone:\")\n",
    "generate_char_response()\n",
    "print()\n",
    "print(\"Without FFNN tone:\")\n",
    "generate_char_response(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs4100)",
   "language": "python",
   "name": "cs4100"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
